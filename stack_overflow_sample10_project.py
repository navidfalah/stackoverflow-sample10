# -*- coding: utf-8 -*-
"""stack overflow sample10 project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gcWelEFJ5DvhRtcHZ3BCBApDTKa0TIr5
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#!/bin/bash
!curl -L -o stacksample.zip https://www.kaggle.com/api/v1/datasets/download/stackoverflow/stacksample

! unzip stacksample.zip

df_q = pd.read_csv('Questions.csv', encoding='ISO-8859-1')
df_t = pd.read_csv('Tags.csv', encoding='ISO-8859-1')

df_q.head(), df_q.shape

df_t.shape, df_t.head()

df_t_grouped = df_t.groupby("Id")["Tag"].apply(list).reset_index()
df_t_grouped.head()

df = pd.merge(df_q, df_t_grouped, on="Id", how="inner")
df.head(), df.shape

df = df.head(10000)

tag_counts = df_t["Tag"].value_counts().head(20)
plt.figure(figsize=(12,6))
tag_counts.plot(kind='bar')
plt.title("Top 20 Most Frequent Tags")
plt.show()

unique_tags = np.unique(df["Tag"].astype(str))
len(unique_tags)

### so we have 3.7 million data and 37k classes that we want to classify them into these calsses
## and we are going to use the transformers of hugging face for this
## multi label classification using transformers pretrained using the finetuning

!pip install -q transformers datasets scikit-learn;

print(df["Tag"].head())
print("\nMissing values:", df["Tag"].isna().sum())

df_cleaned = df[df["Tag"].apply(lambda x: isinstance(x, list) and len(x) > 0)]
df_cleaned.head()
df_cleaned = df_cleaned.drop(columns=str.split("Id	OwnerUserId	CreationDate	ClosedDate	Score"))
df_cleaned.head()

# Clean the Tag column:
df_cleaned["Tag"] = df_cleaned["Tag"].apply(
    lambda tags: [str(tag) for tag in tags if not pd.isna(tag)]
)

# Drop rows with empty tags (optional):
df_cleaned = df_cleaned[df_cleaned["Tag"].apply(len) > 0]

df_cleaned.shape

from sklearn.preprocessing import MultiLabelBinarizer

# Encode tags into a binary matrix
mlb = MultiLabelBinarizer()
tag_matrix = mlb.fit_transform(df_cleaned["Tag"])

print("Encoded tags shape:", tag_matrix.shape)
print("Example tags:", mlb.classes_[:5])  # Show first 5 classes

!pip install iterative-stratification

from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit

# Initialize the splitter
msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

# Perform the split
train_idx, temp_idx = next(msss.split(df, tag_matrix))
train_df, temp_df = df.iloc[train_idx], df.iloc[temp_idx]
train_tags, temp_tags = tag_matrix[train_idx], tag_matrix[temp_idx]

# Split temp into validation and test
msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)
valid_idx, test_idx = next(msss.split(temp_df, temp_tags))
valid_df, test_df = temp_df.iloc[valid_idx], temp_df.iloc[test_idx]
valid_tags, test_tags = temp_tags[valid_idx], temp_tags[test_idx]

train_df.shape, valid_df.shape, test_df.shape

### as we have set the dataframe now we want to go for train

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load pretrained tokenizer and model
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(mlb.classes_),  # Number of unique tags
    problem_type="multi_label_classification"  # Specify multi-label classification
)

### tokenize the text and encode it then train it using the encodings

# Combine Title and Body (if available) into a single text column
train_df["Text"] = train_df["Title"] + " " + train_df["Body"]
valid_df["Text"] = valid_df["Title"] + " " + valid_df["Body"]
test_df["Text"] = test_df["Title"] + " " + test_df["Body"]

# Tokenize the text data
def tokenize_data(texts, max_length=512):
    return tokenizer(
        texts.tolist(),  # Convert to list for tokenizer
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt"  # Return PyTorch tensors
    )

# Tokenize train, validation, and test data
train_encodings = tokenize_data(train_df["Text"])
valid_encodings = tokenize_data(valid_df["Text"])
test_encodings = tokenize_data(test_df["Text"])

import torch
from torch.utils.data import Dataset

class TagDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.float)
        return item

    def __len__(self):
        return len(self.labels)

# Create datasets
train_dataset = TagDataset(train_encodings, train_tags)
valid_dataset = TagDataset(valid_encodings, valid_tags)
test_dataset = TagDataset(test_encodings, test_tags)

from transformers import Trainer, TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",  # Directory to save model checkpoints
    num_train_epochs=3,  # Number of epochs
    per_device_train_batch_size=4,  # Batch size for training
    per_device_eval_batch_size=8,  # Batch size for evaluation
    warmup_steps=500,  # Warmup steps for learning rate scheduler
    weight_decay=0.01,  # Weight decay for regularization
    logging_dir="./logs",  # Directory for logs
    evaluation_strategy="epoch",  # Evaluate every epoch
    save_strategy="epoch",  # Save model every epoch
    learning_rate=2e-5,  # Learning rate
    load_best_model_at_end=True,
    report_to='none'# Load the best model at the end of training
)

# Define the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
)

trainer.train()

model.save_pretrained("./saved_model")

# Evaluate on the validation set
val_results = trainer.evaluate()
print("Validation results:", val_results)

# Evaluate on the test set
test_results = trainer.evaluate(test_dataset)
print("Test results:", test_results)

from huggingface_hub import login

# Log in to Hugging Face
login(token="")

model.push_to_hub("navidfalah/bert-multi-label-classification-stackoverflow-tags")